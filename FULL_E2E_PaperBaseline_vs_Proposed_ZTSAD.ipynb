{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f3dd52",
   "metadata": {},
   "source": [
    "# FULL CODE (End-to-End) — Baseline Paper Model vs Proposed Model (from Energy Reports 2024)\n",
    "\n",
    "Bạn **chỉ có data, chưa làm gì hết** → notebook này làm từ A→Z:\n",
    "\n",
    "✅ Hỗ trợ 2 dạng dữ liệu:\n",
    "1) **UCI** `household_power_consumption.txt`  \n",
    "2) **CSV** có `timestamp` + `power`/`consumption`\n",
    "\n",
    "✅ Pipeline paper Energy Reports 2024:\n",
    "- 15 phút → DLP 96 điểm/ngày  \n",
    "- Weekday pattern (mean/std)  \n",
    "- Z-score  \n",
    "- Feature 99 = 96 Z + max/mean/min  \n",
    "\n",
    "✅ So sánh 3 model:\n",
    "1) **ER-ANN (Baseline theo paper)**  \n",
    "2) **Z-TSAD (Proposed: CNN1D + BiLSTM + Self-Attention + Dual-Branch + score head)**  \n",
    "3) **Z-AE (Unsupervised baseline: Autoencoder anomaly score)**\n",
    "\n",
    "⚠️ Nếu bạn không có nhãn 33 lớp: notebook tạo **pseudo-label** để chạy so sánh end-to-end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f742d782",
   "metadata": {},
   "source": [
    "## 0) Cài thư viện (nếu chạy local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q pandas numpy scikit-learn matplotlib tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1be87",
   "metadata": {},
   "source": [
    "## 1) Chọn loại dữ liệu + đọc dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4900bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_TYPE = \"UCI_TXT\"   # \"UCI_TXT\" hoặc \"CSV\"\n",
    "\n",
    "UCI_PATH = r\"D:\\AIL303c\\data\\household_power_consumption.txt\"   # <-- sửa\n",
    "\n",
    "CSV_PATH = r\"D:\\your_data.csv\"  # <-- sửa\n",
    "CSV_TS_COL = \"timestamp\"\n",
    "CSV_VAL_COL = \"power\"\n",
    "CSV_AGG = \"mean\"  # \"mean\"(kW) hoặc \"sum\"(kWh)\n",
    "\n",
    "def load_data():\n",
    "    if DATA_TYPE == \"UCI_TXT\":\n",
    "        df = pd.read_csv(UCI_PATH, sep=\";\", na_values=\"?\", low_memory=False)\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"], dayfirst=True, errors=\"coerce\")\n",
    "        df = df[[\"timestamp\", \"Global_active_power\"]].dropna()\n",
    "        df = df.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "        s = df[\"Global_active_power\"].astype(float)\n",
    "        return s, \"kW_mean\"\n",
    "    else:\n",
    "        df = pd.read_csv(CSV_PATH)\n",
    "        df[CSV_TS_COL] = pd.to_datetime(df[CSV_TS_COL])\n",
    "        df = df.sort_values(CSV_TS_COL).set_index(CSV_TS_COL)\n",
    "        s = df[CSV_VAL_COL].astype(float)\n",
    "        return s, (\"kWh_sum\" if CSV_AGG==\"sum\" else \"kW_mean\")\n",
    "\n",
    "raw_series, series_kind = load_data()\n",
    "print(\"Loaded series:\", raw_series.shape, \"| kind:\", series_kind)\n",
    "raw_series.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad556dcc",
   "metadata": {},
   "source": [
    "## 2) Resample 15 phút + xử lý missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a330741",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_15m = raw_series.resample(\"15min\").mean() if series_kind == \"kW_mean\" else raw_series.resample(\"15min\").sum()\n",
    "s_15m = s_15m.interpolate(method=\"time\", limit_direction=\"both\")\n",
    "s_15m = s_15m.fillna(s_15m.median())\n",
    "print(\"NaN after cleaning:\", int(s_15m.isna().sum()))\n",
    "s_15m.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e12faf",
   "metadata": {},
   "source": [
    "## 3) DLP 96 điểm/ngày"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29882427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_daily_matrix(series_15m: pd.Series):\n",
    "    days, idx = [], []\n",
    "    for d, chunk in series_15m.groupby(series_15m.index.date):\n",
    "        if len(chunk) < 96:\n",
    "            continue\n",
    "        arr = chunk.iloc[:96].values.astype(float)\n",
    "        if np.isnan(arr).any():\n",
    "            continue\n",
    "        days.append(arr)\n",
    "        idx.append(pd.Timestamp(d))\n",
    "    X_raw = np.vstack(days) if len(days) else np.empty((0, 96))\n",
    "    dates = pd.DatetimeIndex(idx)\n",
    "    return X_raw, dates\n",
    "\n",
    "X_raw, dates = build_daily_matrix(s_15m)\n",
    "print(\"X_raw:\", X_raw.shape, \"| date range:\", dates.min(), \"->\", dates.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128954aa",
   "metadata": {},
   "source": [
    "## 4) Weekday pattern + Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ee2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weekday_patterns(X_raw, dates, min_days=5):\n",
    "    patterns = {}\n",
    "    for w in range(7):\n",
    "        mask = (dates.weekday == w)\n",
    "        Xw = X_raw[mask]\n",
    "        if len(Xw) < min_days:\n",
    "            continue\n",
    "        mu = np.mean(Xw, axis=0)\n",
    "        sd = np.std(Xw, axis=0, ddof=1)\n",
    "        patterns[w] = (mu, sd)\n",
    "    return patterns\n",
    "\n",
    "def z_normalize_by_weekday(X_raw, dates, patterns, eps=1e-6):\n",
    "    Z = np.zeros_like(X_raw, dtype=float)\n",
    "    avail = sorted(patterns.keys())\n",
    "    if not avail:\n",
    "        raise ValueError(\"Không đủ dữ liệu để tạo weekday pattern. Cần nhiều ngày hơn.\")\n",
    "    for i, d in enumerate(dates):\n",
    "        w = int(d.weekday())\n",
    "        if w not in patterns:\n",
    "            w = avail[0]\n",
    "        mu, sd = patterns[w]\n",
    "        sd_safe = np.where((sd < eps) | np.isnan(sd), 1.0, sd)\n",
    "        Zi = (X_raw[i] - mu) / sd_safe\n",
    "        Z[i] = np.nan_to_num(Zi, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return Z\n",
    "\n",
    "patterns = compute_weekday_patterns(X_raw, dates, min_days=5)\n",
    "Z = z_normalize_by_weekday(X_raw, dates, patterns)\n",
    "\n",
    "print(\"patterns weekdays:\", sorted(patterns.keys()))\n",
    "print(\"Z:\", Z.shape, \"| NaN:\", int(np.isnan(Z).sum()))\n",
    "print(\"Z sample:\", Z[0][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cf306e",
   "metadata": {},
   "source": [
    "## 5) Feature 99 + plot 1 ngày"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393dae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_max = X_raw.max(axis=1)\n",
    "daily_mean = X_raw.mean(axis=1)\n",
    "daily_min = X_raw.min(axis=1)\n",
    "\n",
    "X_99 = np.column_stack([Z, daily_max, daily_mean, daily_min])\n",
    "print(\"X_99:\", X_99.shape)\n",
    "\n",
    "day_idx = 0\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(Z[day_idx])\n",
    "plt.title(f\"Z-score DLP (date={dates[day_idx].date()})\")\n",
    "plt.xlabel(\"slot (0..95)\"); plt.ylabel(\"Z\"); plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed0b5f",
   "metadata": {},
   "source": [
    "## 6) Pseudo-label 33 lớp (nếu chưa có label thật)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLOTS = {\n",
    "    \"night\": np.arange(0, 24),\n",
    "    \"early\": np.arange(24, 32),\n",
    "    \"work\":  np.arange(32, 72),\n",
    "    \"eve\":   np.arange(72, 96),\n",
    "}\n",
    "\n",
    "def catalog_33_pseudo(Z, X_raw, z_thr=2.0):\n",
    "    n = Z.shape[0]\n",
    "    Y = np.zeros((n, 33), dtype=int)\n",
    "    out = (np.abs(Z) >= z_thr)\n",
    "    night, early, work, eve = SLOTS[\"night\"], SLOTS[\"early\"], SLOTS[\"work\"], SLOTS[\"eve\"]\n",
    "    nonwork = np.r_[night, early, eve]\n",
    "\n",
    "    for i in range(n):\n",
    "        out_any = out[i]\n",
    "        meanZ, maxZ, minZ = np.mean(Z[i]), np.max(Z[i]), np.min(Z[i])\n",
    "        f_all = np.mean(out_any)\n",
    "        f_work = np.mean(out_any[work])\n",
    "        f_non = np.mean(out_any[nonwork])\n",
    "\n",
    "        if not out_any.any(): Y[i,0] = 1\n",
    "        x = X_raw[i]\n",
    "        if np.mean(np.isclose(np.diff(x), 0.0, atol=1e-4)) > 0.30: Y[i,1] = 1\n",
    "        if f_work > 0.15: Y[i,2] = 1\n",
    "        if out_any[work].any() and (not out_any[nonwork].any()): Y[i,3] = 1\n",
    "        if out_any[work].any() and (not out_any[early].any()): Y[i,4] = 1\n",
    "        if np.mean(out_any[night]) > 0.10 and np.mean(Z[i, night] > z_thr) > 0.05: Y[i,5] = 1\n",
    "        if np.mean(out_any[early]) > 0.10: Y[i,6] = 1\n",
    "        if (not out_any[night].any()) and out_any[work].any(): Y[i,7] = 1\n",
    "        if out_any[night].any(): Y[i,8] = 1\n",
    "        if np.mean(out_any[early]) > 0.25: Y[i,9] = 1\n",
    "        if np.mean(out_any[night]) > 0.25: Y[i,10] = 1\n",
    "        if (f_non > f_work + 0.05) and (abs(meanZ) < 0.2): Y[i,11] = 1; Y[i,12] = 1\n",
    "        if (0 < f_all < 0.05) and (abs(meanZ) < 0.2): Y[i,13] = 1\n",
    "        if (f_work > f_non + 0.05) and (abs(meanZ) < 0.2): Y[i,14] = 1\n",
    "        if meanZ > 0.5: Y[i,15] = 1\n",
    "        if meanZ < -0.5: Y[i,16] = 1\n",
    "        if (np.max(Z[i, work]) >= z_thr) and (np.max(Z[i, eve]) >= z_thr): Y[i,17] = 1\n",
    "        if (np.mean(Z[i, work]) < -0.3) and (np.max(Z[i, work]) < 0.5): Y[i,18] = 1\n",
    "        if (minZ > -0.2) and (np.mean(Z[i] > 0.5) > 0.10): Y[i,19] = 1\n",
    "        if minZ <= -z_thr: Y[i,20] = 1\n",
    "        if meanZ > 0.7: Y[i,21] = 1\n",
    "        if meanZ < -0.7: Y[i,22] = 1\n",
    "        if maxZ >= z_thr: Y[i,23] = 1\n",
    "        if (np.max(Z[i]) < 0.3) and (np.mean(Z[i, work]) < -0.4): Y[i,24] = 1\n",
    "        if minZ >= z_thr: Y[i,25] = 1\n",
    "        if minZ <= -z_thr: Y[i,26] = 1\n",
    "        if np.mean(Z[i, work]) > 0.6: Y[i,27] = 1\n",
    "        if (np.min(Z[i]) > -0.2) and (np.mean(Z[i] > 0.4) > 0.20): Y[i,28] = 1\n",
    "        if np.max(Z[i, work]) > (z_thr + 0.5): Y[i,29] = 1\n",
    "        if np.mean(Z[i, work]) < -0.6: Y[i,30] = 1\n",
    "        if Y[i,25] == 1: Y[i,31] = 1\n",
    "        if Y[i,26] == 1: Y[i,32] = 1\n",
    "\n",
    "    return Y\n",
    "\n",
    "Y_33 = catalog_33_pseudo(Z, X_raw, z_thr=2.0)\n",
    "print(\"Y_33:\", Y_33.shape, \"| avg labels/day:\", float(Y_33.sum(axis=1).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa8a7ff",
   "metadata": {},
   "source": [
    "## 7) Split + Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_99, Y_33, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "print(\"Train:\", X_train_s.shape, y_train.shape, \"| Test:\", X_test_s.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75220500",
   "metadata": {},
   "source": [
    "## 8) Baseline ER-ANN (paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae0f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_er_ann():\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(99,)),\n",
    "        layers.Dense(100, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(33, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[keras.metrics.BinaryAccuracy(name=\"bin_acc\")])\n",
    "    return model\n",
    "\n",
    "er_ann = build_er_ann()\n",
    "hist_er = er_ann.fit(X_train_s, y_train, validation_split=0.2, epochs=30, batch_size=64, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e056612",
   "metadata": {},
   "source": [
    "## 9) Proposed Z-TSAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1290a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_inputs(X99):\n",
    "    z = X99[:, :96].astype(np.float32)\n",
    "    stats = X99[:, 96:].astype(np.float32)\n",
    "    return z[..., None], stats\n",
    "\n",
    "z_train, s_train = split_inputs(X_train_s)\n",
    "z_test,  s_test  = split_inputs(X_test_s)\n",
    "\n",
    "class SelfAttention(layers.Layer):\n",
    "    def __init__(self, d_k=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_k = d_k\n",
    "        self.q = layers.Dense(d_k)\n",
    "        self.k = layers.Dense(d_k)\n",
    "        self.v = layers.Dense(d_k)\n",
    "\n",
    "    def call(self, x):\n",
    "        Q = self.q(x); K = self.k(x); V = self.v(x)\n",
    "        attn = tf.nn.softmax(tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(tf.cast(self.d_k, tf.float32)), axis=-1)\n",
    "        return tf.matmul(attn, V)\n",
    "\n",
    "def build_z_tsad():\n",
    "    inp_z = layers.Input(shape=(96,1), name=\"z_in\")\n",
    "    x = layers.Conv1D(32, 5, padding=\"same\", activation=\"relu\")(inp_z)\n",
    "    x = layers.Conv1D(64, 5, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = SelfAttention(d_k=64)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    inp_s = layers.Input(shape=(3,), name=\"stats_in\")\n",
    "    s = layers.Dense(16, activation=\"relu\")(inp_s)\n",
    "\n",
    "    h = layers.Concatenate()([x, s])\n",
    "    h = layers.Dense(128, activation=\"relu\")(h)\n",
    "    h = layers.Dropout(0.4)(h)\n",
    "\n",
    "    out33 = layers.Dense(33, activation=\"sigmoid\", name=\"out33\")(h)\n",
    "    score = layers.Dense(1, activation=\"sigmoid\", name=\"score\")(h)\n",
    "\n",
    "    model = keras.Model(inputs=[inp_z, inp_s], outputs=[out33, score], name=\"Z_TSAD\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.005),\n",
    "                  loss={\"out33\":\"binary_crossentropy\",\"score\":\"binary_crossentropy\"},\n",
    "                  loss_weights={\"out33\":1.0,\"score\":0.3},\n",
    "                  metrics={\"out33\":[keras.metrics.BinaryAccuracy(name=\"bin_acc\")]})\n",
    "    return model\n",
    "\n",
    "score_train = (y_train.sum(axis=1) > y_train[:,0]).astype(int).reshape(-1,1)\n",
    "score_test  = (y_test.sum(axis=1) > y_test[:,0]).astype(int).reshape(-1,1)\n",
    "\n",
    "z_tsad = build_z_tsad()\n",
    "hist_tsad = z_tsad.fit({\"z_in\":z_train,\"stats_in\":s_train},\n",
    "                       {\"out33\":y_train,\"score\":score_train},\n",
    "                       validation_split=0.2, epochs=30, batch_size=64, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8348b3b1",
   "metadata": {},
   "source": [
    "## 10) Unsupervised Z-AE (Autoencoder score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_z_autoencoder():\n",
    "    inp = layers.Input(shape=(96,), name=\"z_vec\")\n",
    "    x = layers.Dense(64, activation=\"relu\")(inp)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    b = layers.Dense(16, activation=\"relu\")(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(b)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    out = layers.Dense(96, activation=\"linear\")(x)\n",
    "    ae = keras.Model(inp, out, name=\"Z_AE\")\n",
    "    ae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "    return ae\n",
    "\n",
    "Z_train = X_train_s[:, :96].astype(np.float32)\n",
    "Z_test  = X_test_s[:, :96].astype(np.float32)\n",
    "\n",
    "z_ae = build_z_autoencoder()\n",
    "hist_ae = z_ae.fit(Z_train, Z_train, validation_split=0.2, epochs=30, batch_size=128, verbose=1)\n",
    "\n",
    "recon = z_ae.predict(Z_test, verbose=0)\n",
    "ae_score = np.mean((Z_test - recon)**2, axis=1)\n",
    "print(\"AE score sample:\", ae_score[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8773443",
   "metadata": {},
   "source": [
    "## 11) Evaluation (Precision/FPR/FNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ed8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def per_class_metrics(y_true, y_hat):\n",
    "    rows = []\n",
    "    for j in range(y_true.shape[1]):\n",
    "        yt, yp = y_true[:, j], y_hat[:, j]\n",
    "        TP = np.sum((yt==1) & (yp==1))\n",
    "        FP = np.sum((yt==0) & (yp==1))\n",
    "        FN = np.sum((yt==1) & (yp==0))\n",
    "        TN = np.sum((yt==0) & (yp==0))\n",
    "        precision = TP / (TP + FP + 1e-9)\n",
    "        fpr = FP / (FP + TN + 1e-9)\n",
    "        fnr = FN / (FN + TP + 1e-9)\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN + 1e-9)\n",
    "        rows.append([j+1, acc, precision, fpr, fnr, int(yt.sum())])\n",
    "    return pd.DataFrame(rows, columns=[\"class\",\"accuracy\",\"precision\",\"FPR\",\"FNR\",\"support\"])\n",
    "\n",
    "prob_er = er_ann.predict(X_test_s, verbose=0)\n",
    "pred_er = (prob_er >= 0.5).astype(int)\n",
    "\n",
    "prob_tsad, prob_score = z_tsad.predict({\"z_in\":z_test,\"stats_in\":s_test}, verbose=0)\n",
    "pred_tsad = (prob_tsad >= 0.5).astype(int)\n",
    "\n",
    "micro_er = precision_score(y_test.reshape(-1), pred_er.reshape(-1), zero_division=0)\n",
    "micro_tsad = precision_score(y_test.reshape(-1), pred_tsad.reshape(-1), zero_division=0)\n",
    "\n",
    "print(\"Micro Precision ER-ANN:\", round(float(micro_er), 4))\n",
    "print(\"Micro Precision Z-TSAD:\", round(float(micro_tsad), 4))\n",
    "\n",
    "m_er = per_class_metrics(y_test, pred_er).sort_values(\"support\", ascending=False)\n",
    "m_ts = per_class_metrics(y_test, pred_tsad).sort_values(\"support\", ascending=False)\n",
    "\n",
    "display(m_er.head(10))\n",
    "display(m_ts.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea4f74",
   "metadata": {},
   "source": [
    "## 12) Evaluation anomaly score (AUC/AUPRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6378b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "y_bin = (y_test.sum(axis=1) > y_test[:,0]).astype(int)\n",
    "\n",
    "tsad_score = prob_score.reshape(-1)\n",
    "\n",
    "ae_auc = roc_auc_score(y_bin, ae_score)\n",
    "ae_auprc = average_precision_score(y_bin, ae_score)\n",
    "\n",
    "ts_auc = roc_auc_score(y_bin, tsad_score)\n",
    "ts_auprc = average_precision_score(y_bin, tsad_score)\n",
    "\n",
    "print(\"Z-AE  AUC:\", round(float(ae_auc), 4), \"| AUPRC:\", round(float(ae_auprc), 4))\n",
    "print(\"Z-TSAD AUC:\", round(float(ts_auc), 4), \"| AUPRC:\", round(float(ts_auprc), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd79b5",
   "metadata": {},
   "source": [
    "## 13) Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist_er.history[\"bin_acc\"], label=\"ER train bin_acc\")\n",
    "plt.plot(hist_er.history[\"val_bin_acc\"], label=\"ER val bin_acc\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"bin_acc\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist_tsad.history[\"out33_bin_acc\"], label=\"TSAD train bin_acc\")\n",
    "plt.plot(hist_tsad.history[\"val_out33_bin_acc\"], label=\"TSAD val bin_acc\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"bin_acc\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff4ad5",
   "metadata": {},
   "source": [
    "## 14) Export bảng kết quả CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e72e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame({\n",
    "    \"model\": [\"ER-ANN (paper)\", \"Z-TSAD (proposed)\"],\n",
    "    \"micro_precision\": [micro_er, micro_tsad],\n",
    "    \"anomaly_AUC\": [np.nan, ts_auc],\n",
    "    \"anomaly_AUPRC\": [np.nan, ts_auprc],\n",
    "})\n",
    "display(summary)\n",
    "\n",
    "m_er.to_csv(\"metrics_er_ann.csv\", index=False)\n",
    "m_ts.to_csv(\"metrics_z_tsad.csv\", index=False)\n",
    "summary.to_csv(\"summary_metrics.csv\", index=False)\n",
    "print(\"Saved: metrics_er_ann.csv, metrics_z_tsad.csv, summary_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45777025",
   "metadata": {},
   "source": [
    "## 15) Checklist để bạn viết paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812f73ab",
   "metadata": {},
   "source": [
    "- Sửa `DATA_TYPE` + đường dẫn file  \n",
    "- Chạy từ Cell 1 → 14 để lấy bảng kết quả + biểu đồ  \n",
    "- Viết paper: Baseline (ER-ANN) vs Proposed (Z-TSAD) + Unsupervised (Z-AE)  \n",
    "- Nếu bạn có nhãn thật: thay phần pseudo-label bằng nhãn thật để paper “chuẩn” hơn\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
